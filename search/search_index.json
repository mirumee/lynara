{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Lynara is a tool that allows running Python ASGI applications in an AWS Lambda runtime.</p> <p>When writing Lynara, we closely followed what Uvicorn does to fulfill its role as an ASGI server. However, instead of running in a long-lived loop serving many requests, Lynara receives an AWS event, passes it to the application, and stops.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Install Lynara:</p> Poetrypip <pre><code>poetry add lynara\n</code></pre> <pre><code>pip install lynara\n</code></pre> <p>Use Lynara with your ASGI application:</p> app.py<pre><code>import asyncio\nfrom lynara import Lynara, APIGatewayProxyEventV2Interface\nfrom fastapi import FastAPI\n\napp = FastAPI()# (1)!\nlynara = Lynara(app=app)\n\ndef lambda_handler(event, context):\n    return asyncio.run(\n        lynara.run(event, context, APIGatewayProxyEventV2Interface)\n    )\n</code></pre> <ol> <li>The <code>app</code> is loaded once for every cold start.</li> </ol>"},{"location":"#rationale","title":"Rationale","text":"<p>We wanted to quickly deploy small and scalable Python applications on Lambdas. Our goal was to support small FastAPI applications with a few endpoints, but not as small as a single lambda handler. Leveraging Starlette or FastAPI in a serverless runtime was an appealing idea. Although there are existing solutions like Mangum and aws-lambda-web-adapter, they did not meet our needs.</p> <p>Another aspect is being able to jump off a serverless environment as Lambdas are cheaper than Fargates and EC2s only up to a certain point. Here's a neat AWS Cost Estimator that might help you understand the costs.</p> <p>What started as an evening experiment has resulted in a functional tool. Please star it on GitHub, share feedback, and follow the project if you're interested.</p>"},{"location":"#others-from-mirumee","title":"Others from Mirumee","text":"<ul> <li>Smyth - A tool improving the Lambda developer experience</li> <li>Ariadne - Schema-first, Python GraphQL server</li> <li>Ariadne Codegen - GraphQL Python code generator</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We developed a benchmarking suite to test if Lynara causes any penalties to the load time of a Lambda. Lynara itself does not, but there are some interesting results worth sharing.</p>"},{"location":"benchmarks/#tests","title":"Tests","text":"<p>We prepared four different applications doing the same thing: receiving a <code>POST</code> request with a <code>{\"name\": \"Ana\"}</code> payload and outputting a <code>\"Hello Ana\"</code> string.</p> <p>Notice</p> <p>It's important to note that FastAPI and Django are doing a bit more than the \"pure lambda\" applications, such as checking if the HTTP method is indeed a <code>POST</code>, whereas the others do not.</p> Pure LambdaPure Async LambdaFastAPIStarletteDjango <p>To set a baseline, we tested a basic Lambda:</p> <pre><code>import json\n\ndef lambda_handler(event, context):\n    json_data = json.loads(event['body'])\n    res = {\n        \"statusCode\": 200,\n        \"headers\": {\n            \"Content-Type\": \"*/*\"\n        },\n        \"body\": f\"Hello {json_data.get('name', 'Unknown')}\"\n    }\n    return res\n</code></pre> <p>A somewhat controversial setup forcing an async loop:</p> <pre><code>import asyncio\nimport json\n\nasync def handler(event, context):\n    json_data = json.loads(event['body'])\n    res = {\n        \"statusCode\": 200,\n        \"headers\": {\n            \"Content-Type\": \"*/*\"\n        },\n        \"body\": f\"Hello {json_data.get('name', 'Unknown')}\"\n    }\n    return res\n\ndef lambda_handler(event, context):\n    return asyncio.run(handler(event, context))\n</code></pre> <pre><code>import asyncio\nfrom fastapi import FastAPI\nfrom lynara import Lynara, APIGatewayProxyEventV1Interface, APIGatewayProxyEventV2Interface\n\napp = FastAPI()\nlynara = Lynara(app=app)\n\n@app.post(\"/hello\")\nasync def hello(payload: dict):\n    return {\"data\": f\"Hello {payload.get('name', 'Unknown')\"}\n\ndef lambda_handler_v2(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV2Interface))\n\ndef lambda_handler_v1(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV1Interface))\n</code></pre> <pre><code>import asyncio\nimport json\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom lynara import Lynara, APIGatewayProxyEventV1Interface, APIGatewayProxyEventV2Interface\n\nasync def hello(request):\n    payload = await request.json()\n    return JSONResponse({\"data\": f\"Hello {payload.get('name', 'Unknown')}\"})\n\napp = Starlette(routes=[Route('/hello', hello, methods=['POST'])])\nlynara = Lynara(app=app)\n\ndef lambda_handler_v2(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV2Interface))\n\ndef lambda_handler_v1(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV1Interface))\n</code></pre> <pre><code>import asyncio\nimport json\nimport os\nimport sys\nfrom django.conf import settings\nfrom django.core.asgi import get_asgi_application\nfrom django.http import JsonResponse\nfrom django.urls import path\nfrom django.utils.crypto import get_random_string\nfrom django.views.decorators.http import require_http_methods\nfrom lynara import Lynara, APIGatewayProxyEventV1Interface, APIGatewayProxyEventV2Interface\n\nsettings.configure(\n    DEBUG=(os.environ.get(\"DEBUG\", \"\") == \"1\"),\n    ALLOWED_HOSTS=[\"*\"],\n    ROOT_URLCONF=__name__,\n    SECRET_KEY=get_random_string(50),\n    MIDDLEWARE=[\"django.middleware.common.CommonMiddleware\"],\n)\n\n@require_http_methods([\"POST\"])\ndef hello_world(request):\n    body = json.loads(request.body)\n    return JsonResponse({\"data\": f\"Hello {body.get('name', 'Unknown')}\"})\n\nurlpatterns = [\n    path(\"hello\", hello_world),\n]\napplication = get_asgi_application()\nlynara = Lynara(app=application)\n\ndef lambda_handler_v2(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV2Interface))\n\ndef lambda_handler_v1(event, context):\n    return asyncio.run(lynara.run(event, context, APIGatewayProxyEventV1Interface))\n</code></pre> <p>The test was conducted with Locust making POST requests to AWS Lambdas from an M1 MacBook Pro with 1000 concurrent users. The Lambda setup:</p> <ul> <li>Default 1000 concurrent lambda invocation per account cap</li> <li>128MB Memory</li> <li>x86 runtime</li> <li>Zip upload (no containers)</li> </ul>"},{"location":"benchmarks/#results","title":"Results","text":"<p>Limited tests</p> <p>The test machine could not exhaust the capabilities of AWS. These benchmarks set some expectations but should not be used for cost savings or other financial decisions. You should do your own measurements for your specific application.</p> <p>Use the tabs below to view the data in charts.</p> TableRPS / EfficiencyRPS / ExecutionsFirst Response <p> Setup Proxy RPS (higher is better) Lambda Concurrent Executions (lower is better) First Response After (lower is better) Efficiency per Invocation (higher is better) Pure Lambda V1 (API Gateway) 2865.1 324 avg 0.43s (max 0.47s, min 0.39s) 8.42 Pure Lambda V2 (Function URL) 2884.3 264 avg 0.49s (max 0.56s, min 0.44s) 10.92 Lynara + Starlette V1 (API Gateway) 2806.6 327 avg 0.66s (max 0.79s, min 0.18s) 7.54 Lynara + Starlette V2 (Function URL) 2845.7 328 avg 0.67s (max 0.81s, min 0.20s) 8.67 Lynara + FastAPI V1 (API Gateway) 2769.4 362 avg 1.50s (max 1.69s, min 1.17s) 7.65 Lynara + FastAPI V2 (Function URL) 2871.1 317 avg 1.54s (max 1.76s, min 1.23s) 9.05 Lynara + Django V1 (API Gateway) 2560.8 684 avg 1.34s (max 1.44s, min 1.08s) 3.74 Lynara + Django V2 (Function URL) 2751.0 638 avg 1.44s (max 1.56s, min 1.33s) 4.31 Pure Async Lambda V1 (API Gateway) 2752.9 339 avg 0.55s (max 0.74s, min 0.48s) 8.12 Pure Async Lambda V2 (Function URL) 2906.7 307 avg 0.54s (max 0.84s, min 0.25s) 9.46 </p> <p>Measuring the cost of ownership of such Lambdas, we can see that (unless we're using Django) we are able to achieve similar traffic throughput at a similar cost when using the ASGI frameworks as we would with a \"Pure\" Lambda.</p> <p>{     \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",     \"background\": \"#00000000\",     \"height\": 500,     \"data\": {         \"url\": \"data.json\",         \"format\": {             \"type\": \"json\"         }     },     \"resolve\": {\"scale\": {\"y\": \"independent\"}},     \"layer\": [         {             \"mark\": {                 \"type\": \"bar\",                 \"clip\": true             },             \"encoding\": {                 \"x\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"setup\",                     \"title\": \"Setup\"                 },                 \"y\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"rps\",                     \"title\": \"Requests Per Second\",                     \"type\": \"quantitative\",                     \"scale\": {                         \"domain\": [                             2500,                             2950                         ],                         \"zero\": false                     }                 },                 \"xOffset\": {                     \"field\": \"proxy\"                 },                 \"color\": {                     \"field\": \"proxy\",                     \"labelFontSize\": 20                 }             }         },         {             \"mark\": {                 \"type\": \"line\",                 \"stroke\": \"#f1c36f\"             },             \"encoding\": {                 \"x\": {                     \"field\": \"setup\"                 },                 \"xOffset\": {                     \"field\": \"proxy\"                 },                 \"y\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"efficiencyPerInvocation\",                     \"type\": \"quantitative\",                     \"title\": \"Efficiency\"                 }             }         }     ] }</p> <p>Django not only handled the least amount of traffic but was also the most costly.</p> <p>{     \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",     \"background\": \"#00000000\",     \"height\": 500,     \"data\": {         \"url\": \"data.json\",         \"format\": {             \"type\": \"json\"         }     },     \"resolve\": {\"scale\": {\"y\": \"independent\"}},     \"layer\": [         {             \"mark\": {                 \"type\": \"bar\",                 \"clip\": true             },             \"encoding\": {                 \"x\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"setup\",                     \"title\": \"Setup\"                 },                 \"y\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"rps\",                     \"title\": \"Requests Per Second\",                     \"type\": \"quantitative\",                     \"scale\": {                         \"domain\": [                             2500,                             2950                         ],                         \"zero\": false                     }                 },                 \"xOffset\": {                     \"field\": \"proxy\"                 },                 \"color\": {                     \"field\": \"proxy\",                     \"labelFontSize\": 20                 }             }         },         {             \"mark\": {                 \"type\": \"line\",                 \"stroke\": \"#e6695b\"             },             \"encoding\": {                 \"x\": {                     \"field\": \"setup\"                 },                 \"xOffset\": {                     \"field\": \"proxy\"                 },                 \"y\": {                     \"axis\": {                         \"titleFontSize\": 24,                         \"labelFontSize\": 20                     },                     \"field\": \"lambdaConcurrentExecutions\",                     \"type\": \"quantitative\",                     \"title\": \"Executions\"                 }             }         }     ] }</p> <p>Response times are best and most consistent for the most basic Lambda. Starlette was not far behind, whereas Django and FastAPI took the longest to start.</p> <p>{     \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",     \"background\": \"#00000000\",     \"height\": 500,     \"data\": {         \"url\": \"data.json\",         \"format\": {             \"type\": \"json\"         }     },     \"encoding\": {         \"x\": {             \"axis\": {                 \"domain\": false,                 \"titleFontSize\": 24,                 \"labelFontSize\": 20             },             \"field\": \"setup\",             \"title\": \"Setup\"         },         \"xOffset\": {             \"field\": \"proxy\"         },         \"y\": {             \"type\": \"quantitative\",             \"scale\": {\"domain\": [0, 2]},             \"axis\": {\"title\": \"Time (s)\",                          \"titleFontSize\": 24,                         \"labelFontSize\": 20}         },         \"color\": {             \"field\": \"proxy\",             \"labelFontSize\": 20         }     },     \"layer\": [         {             \"mark\": {                 \"type\": \"point\",                 \"shape\": \"square\",                 \"color\": \"#fff\",                 \"size\": 100             },             \"encoding\": {                 \"y\": {                     \"field\": \"firstResponseAfter.avg\"                 }             }         },         {             \"mark\": {                 \"type\": \"bar\",                 \"clip\": true,                 \"width\": 3,                 \"color\": \"#fff\"             },             \"encoding\": {                 \"y\": {                     \"field\": \"firstResponseAfter.min\"                 },                 \"y2\": {                     \"field\": \"firstResponseAfter.max\"                 }             }         }     ] }</p> <p>There is a clear penalty on the cold start time when \"bigger\" frameworks need to load for the first time. This tells us that there should be a good reason to use a heavier framework, such as time to market or a team's ability to use a certain framework.</p>"},{"location":"interfaces/interfaces/","title":"Interfaces","text":"<p>Interfaces are classes responsible for the translation of a Lambda event into an ASGI event or request. Those also fulfill the ASGI contract by providing the <code>scope</code> data and <code>receive</code> and <code>send</code> coroutines. </p> <p>Currently supported AWS Events are:</p> Event Description AWS API Gateway Proxy V2 Referred to as HTTP <sup>1</sup>. AWS API Gateway Proxy V1 Referred to as REST <sup>1</sup>. Lambda function URL Supported as it's the same as the V2 gateway payload <sup>2</sup>."},{"location":"interfaces/interfaces/#inner-workings","title":"Inner workings","text":""},{"location":"interfaces/interfaces/#initialization","title":"Initialization","text":"<p>An instantiated interface is provided with the instance of the ASGI app, the lambda event and lambda context. Builds the body of the request that will be sent to the application and queues it in it's <code>app_queue</code>.</p> <p>Also here is where the default response body returned by the lambda will start to take shape.</p> my_interface.py<pre><code>from lynara import HTTPInterface\nfrom lynara.types import ASGIApp, LambdaEvent\n\n\nclass MyEventInterface(HTTPInterface):\n\n    def __init__(self, app: ASGIApp, event: LambdaEvent, context) -&gt; None:\n        super().__init__(app, event, context)\n        self.lambda_response = {\n            \"cookies\": [],\n            \"isBase64Encoded\": False,\n            \"statusCode\": 200,\n            \"body\": \"\",\n            \"headers\": {},\n        }\n\n        body = b\"body generated from event and context\"\n\n        self.app_queue.put_nowait(# (1)!\n            {\n                \"type\": \"http.request\",\n                \"body\": body,\n                \"more_body\": False,\n            }\n        )\n</code></pre> <ol> <li>The <code>self.app_queue</code> is the medium use to transfer messages from and to your ASGI application. Here is a beginning of a HTTP request prepared for the application to pick up when ready. </li> </ol>"},{"location":"interfaces/interfaces/#scope","title":"Scope","text":"<p>Having the event and context on <code>self</code> generate a ASGI HTTP Connection Scope. You need to carefully check which data is available in the Lambda event and map it to the scope object.</p> my_interface.py<pre><code>    @property\n    def scope(self) -&gt; Scope:\n        headers = {k.lower(): v for k, v in self.event.get(\"headers\", {}).items()}\n        request_context = self.event[\"requestContext\"]\n        path = request_context[\"http\"][\"path\"]\n        self._method = request_context[\"http\"][\"method\"]\n\n        return {\n            \"type\": \"http\",\n            \"asgi\": {\n                \"version\": \"3.0\",\n                \"spec_version\": \"2.3\",\n            },\n            \"http_version\": \"1.1\",\n            \"method\": self._method,\n            \"scheme\": headers.get(\"x-forwarded-proto\", \"https\"),\n            \"path\": strip_api_gateway_path(path, base_path=self.base_path),\n            \"raw_path\": None,\n            \"query_string\": self.event.get(\"rawQueryString\", \"\").encode(),\n            \"root_path\": \"\",\n            \"headers\": [(k.encode(), v.encode()) for k, v in headers.items()],\n            \"client\": (request_context[\"http\"][\"sourceIp\"], 0),\n            \"server\": get_server(headers=headers),\n        }\n</code></pre>"},{"location":"interfaces/interfaces/#receive","title":"Receive","text":"<p>This is the coroutine used by the application when it's ready to receive a request. The dict that was prepared on initialization will be \"sent to the application\" here. This is the only body your application will receive - which is exactly what you are after in the single event - single lifetime environment like a lambda. </p> <p>Note about the single event - single lifetime</p> <p>This is not tested yet but in theory it would be possible to handle batch events with Lynara as well, taking SQS for an example - if an event with many messages in a batch would be queued here, then each message could be received by the ASGI app as a request (assuming that an HTTP scope can be made out of an SQS event).</p> my_interface.py<pre><code>    async def receive(self) -&gt; Message:\n        return await self.app_queue.get()\n</code></pre>"},{"location":"interfaces/interfaces/#send","title":"Send","text":"<p>Coroutine used by the application to write a response. It can be invoked many times for one request with the response headers and the body which can be chunked. Finally it queues a <code>http.disconnect</code> message that informs the application that the client (our lambda handler) is done with it.</p> my_interface.py<pre><code>    async def send(self, message: Message) -&gt; None:\n        if message[\"type\"] == \"http.response.start\":\n            self.lambda_response[\"statusCode\"] = message[\"status\"]\n            for key, value in message.get(\"headers\", []):\n                if key.decode().lower() == \"set-cookie\":#(1)!\n                    self.lambda_response[\"cookies\"].append(value.decode())\n                else:\n                    self.lambda_response[\"headers\"][key.decode()] = value.decode()\n\n        elif message[\"type\"] == \"http.response.body\":\n            self.lambda_response[\"body\"] += message.get(\"body\", b\"\").decode()\n            more_body = message.get(\"more_body\", False)\n\n            if not more_body:\n                await self.app_queue.put({\"type\": \"http.disconnect\"})\n\n        else:\n            raise ValueError(f\"Unknown message type: {message['type']}\")\n</code></pre> <ol> <li>API Gateway V2 needs cookies in a separate response field so we need to extract those here</li> </ol> <p>This is where your interface would build the lambda response.</p>"},{"location":"interfaces/interfaces/#match","title":"Match","text":"<p>This class method will be used in the future in a guessing mechanism that could infer the interface based on the shape of the incoming event.</p> <ol> <li> <p>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-integrations-lambda.html\u00a0\u21a9\u21a9</p> </li> <li> <p>https://docs.aws.amazon.com/lambda/latest/dg/urls-invocation.html#urls-payloads\u00a0\u21a9</p> </li> </ol>"},{"location":"interfaces/lifespan/","title":"Lifespan","text":"<p>Lynara handles the lifespan protocol of ASGI with it's LifespanInterface. LifespanInterface while being an interface it's quite different from the HTTPInterfaces both in it's role and behavior.</p> <p>The <code>Lynara.run</code> utility allows to specify which lifespan mode should be used</p> <pre><code>from lynara import LifespanMode, Lynara\n\nlynara = Lynara(app=django_asgi_app, lifespan_mode=LifespanMode.AUTO)\n</code></pre> <p>There are 3 modes:</p> <ul> <li><code>OFF</code> - the lifespan interface will not be used at all</li> <li><code>ON</code> - the lifespan interface will be used, if the application fails to handle it, it will error out</li> <li><code>AUTO</code> - similar to ON, but will not fail if the application does not handle lifespans </li> </ul>"}]}